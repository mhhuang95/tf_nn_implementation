{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate = 0.01\n",
    "batch_size = 256\n",
    "num_steps = 30000\n",
    "\n",
    "step_show = 1000\n",
    "\n",
    "num_input = 28*28\n",
    "num_hidden1 = 256\n",
    "num_hidden2 = 128\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1' : tf.Variable(tf.random_normal([num_input, num_hidden1])),\n",
    "    'encoder_h2' : tf.Variable(tf.random_normal([num_hidden1, num_hidden2])),\n",
    "    'decoder_h1' : tf.Variable(tf.random_normal([num_hidden2, num_hidden1])),\n",
    "    'decoder_h2' : tf.Variable(tf.random_normal([num_hidden1, num_input]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1' : tf.Variable(tf.random_normal([num_hidden1])),\n",
    "    'encoder_b2' : tf.Variable(tf.random_normal([num_hidden2])),\n",
    "    'decoder_b1' : tf.Variable(tf.random_normal([num_hidden1])),\n",
    "    'decoder_b2' : tf.Variable(tf.random_normal([num_input]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, weights, biases):\n",
    "    \n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    out = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return out\n",
    "\n",
    "def decoder(x, weights, biases):\n",
    "    \n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    out = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return out\n",
    "\n",
    "encode = encoder(X, weights, biases)\n",
    "decode = decoder(encode, weights, biases)\n",
    "\n",
    "y_pred = decode\n",
    "y_true = X\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.pow(y_pred - y_true,2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 1000 minibatch loss 0.124391206\n",
      "steps 2000 minibatch loss 0.1111151\n",
      "steps 3000 minibatch loss 0.09972587\n",
      "steps 4000 minibatch loss 0.093297236\n",
      "steps 5000 minibatch loss 0.09082658\n",
      "steps 6000 minibatch loss 0.08831834\n",
      "steps 7000 minibatch loss 0.0864163\n",
      "steps 8000 minibatch loss 0.08709964\n",
      "steps 9000 minibatch loss 0.08357052\n",
      "steps 10000 minibatch loss 0.08245178\n",
      "steps 11000 minibatch loss 0.08096061\n",
      "steps 12000 minibatch loss 0.0787511\n",
      "steps 13000 minibatch loss 0.07919399\n",
      "steps 14000 minibatch loss 0.08084198\n",
      "steps 15000 minibatch loss 0.076705225\n",
      "steps 16000 minibatch loss 0.07673043\n",
      "steps 17000 minibatch loss 0.07295822\n",
      "steps 18000 minibatch loss 0.072882794\n",
      "steps 19000 minibatch loss 0.07086661\n",
      "steps 20000 minibatch loss 0.06763664\n",
      "steps 21000 minibatch loss 0.067439705\n",
      "steps 22000 minibatch loss 0.0678641\n",
      "steps 23000 minibatch loss 0.065185614\n",
      "steps 24000 minibatch loss 0.06583718\n",
      "steps 25000 minibatch loss 0.06532107\n",
      "steps 26000 minibatch loss 0.06505768\n",
      "steps 27000 minibatch loss 0.06365621\n",
      "steps 28000 minibatch loss 0.063958675\n",
      "steps 29000 minibatch loss 0.062173635\n",
      "steps 30000 minibatch loss 0.061924104\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _, loss = sess.run([train_op, loss_op], feed_dict = {X:batch_x})\n",
    "        \n",
    "        if step% step_show == 0:\n",
    "            print(\"steps\", step, \"minibatch loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
