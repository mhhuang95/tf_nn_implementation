{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting \tmp\\data/train-images-idx3-ubyte.gz\n",
      "Extracting \tmp\\data/train-labels-idx1-ubyte.gz\n",
      "Extracting \tmp\\data/t10k-images-idx3-ubyte.gz\n",
      "Extracting \tmp\\data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"\\tmp\\data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002\n",
    "batch_size = 128\n",
    "num_steps = 10000\n",
    "display_steps = 1000\n",
    "\n",
    "image_dim = 28*28\n",
    "hidden_gen = 256\n",
    "hidden_dis = 256\n",
    "noise_dim = 100\n",
    "\n",
    "def glorot_init(shape):\n",
    "    return tf.random_normal(shape, stddev=1. / tf.sqrt(shape[0] / 2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'gen_h1': tf.Variable(glorot_init([noise_dim, hidden_gen])),\n",
    "    'gen_out': tf.Variable(glorot_init([hidden_gen, image_dim])),\n",
    "    'dis_h1': tf.Variable(glorot_init([image_dim, hidden_dis])),\n",
    "    'dis_out': tf.Variable(glorot_init([hidden_dis, 1]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'gen_b1': tf.Variable(glorot_init([hidden_gen])),\n",
    "    'gen_out': tf.Variable(glorot_init([image_dim])),\n",
    "    'dis_b1': tf.Variable(glorot_init([hidden_dis])),\n",
    "    'dis_out': tf.Variable(glorot_init([1]))\n",
    "}\n",
    "\n",
    "def generator(noise):\n",
    "    layer1 = tf.matmul(noise, weights['gen_h1']) + biases['gen_b1']\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    out = tf.matmul(layer1, weights['gen_out']) + biases['gen_out']\n",
    "    out = tf.nn.sigmoid(out)\n",
    "    return out\n",
    "\n",
    "def discriminator(image):\n",
    "    layer1 = tf.matmul(image, weights['dis_h1']) + biases['dis_b1']\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    out = tf.matmul(layer1, weights['dis_out']) + biases['dis_out']\n",
    "    out = tf.nn.sigmoid(out)\n",
    "    return out\n",
    "\n",
    "input_gen = tf.placeholder(tf.float32, [None, noise_dim])\n",
    "input_dis = tf.placeholder(tf.float32, [None, image_dim])\n",
    "\n",
    "gen_sample = generator(input_gen)\n",
    "\n",
    "dis_real = discriminator(input_dis)\n",
    "dis_fake = discriminator(gen_sample)\n",
    "\n",
    "loss_gen = -tf.reduce_mean(tf.log(dis_fake))\n",
    "loss_dis = -tf.reduce_mean(tf.log(dis_real) + tf.log(1. - dis_fake))\n",
    "\n",
    "\n",
    "optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer_dis = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "gen_vars = [weights['gen_h1'], weights['gen_out'],\n",
    "            biases['gen_b1'], biases['gen_out']]\n",
    "dis_vars = [weights['dis_h1'], weights['dis_out'],\n",
    "            biases['dis_b1'], biases['dis_out']]\n",
    "\n",
    "train_gen = optimizer_gen.minimize(loss_gen, var_list=gen_vars)\n",
    "train_dis = optimizer_dis.minimize(loss_dis, var_list=dis_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Generator Loss: 0.023015, Discriminator Loss: 3.869399\n",
      "Step 1000: Generator Loss: 2.524072, Discriminator Loss: 0.280537\n",
      "Step 2000: Generator Loss: 3.537890, Discriminator Loss: 0.066817\n",
      "Step 3000: Generator Loss: 2.323431, Discriminator Loss: 0.239935\n",
      "Step 4000: Generator Loss: 2.622757, Discriminator Loss: 0.205005\n",
      "Step 5000: Generator Loss: 3.138986, Discriminator Loss: 0.331748\n",
      "Step 6000: Generator Loss: 3.143493, Discriminator Loss: 0.219871\n",
      "Step 7000: Generator Loss: 2.682419, Discriminator Loss: 0.237796\n",
      "Step 8000: Generator Loss: 3.414693, Discriminator Loss: 0.149640\n",
      "Step 9000: Generator Loss: 3.051258, Discriminator Loss: 0.207871\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(1, num_steps):\n",
    "        batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        z = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\n",
    "        \n",
    "        feed_dict = {input_gen: z, input_dis: batch_x}\n",
    "        _,_,gl,dl = sess.run([train_gen, train_dis, loss_gen, loss_dis], feed_dict = feed_dict)\n",
    "        \n",
    "        if step % display_steps == 0 or step == 1:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (step, gl, dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
