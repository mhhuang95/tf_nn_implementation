{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   workclass  fnlwgt      education  education-num       marital-status  \\\n",
       "0   25     Private  226802           11th              7        Never-married   \n",
       "1   38     Private   89814        HS-grad              9   Married-civ-spouse   \n",
       "2   28   Local-gov  336951     Assoc-acdm             12   Married-civ-spouse   \n",
       "3   44     Private  160323   Some-college             10   Married-civ-spouse   \n",
       "4   18           ?  103497   Some-college             10        Never-married   \n",
       "\n",
       "           occupation relationship    race      sex  capital-gain  \\\n",
       "0   Machine-op-inspct    Own-child   Black     Male             0   \n",
       "1     Farming-fishing      Husband   White     Male             0   \n",
       "2     Protective-serv      Husband   White     Male             0   \n",
       "3   Machine-op-inspct      Husband   Black     Male          7688   \n",
       "4                   ?    Own-child   White   Female             0   \n",
       "\n",
       "   capital-loss  hours-per-week  native-country   labels  \n",
       "0             0              40   United-States   <=50K.  \n",
       "1             0              50   United-States   <=50K.  \n",
       "2             0              40   United-States    >50K.  \n",
       "3             0              40   United-States    >50K.  \n",
       "4             0              30   United-States   <=50K.  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_data = pd.read_csv(\"adult.data\", sep=',', \n",
    "                         header=None, names=['age','workclass','fnlwgt','education','education-num', 'marital-status', 'occupation',\n",
    "                                             'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "                                             'native-country', 'labels'])\n",
    "adult_test = pd.read_csv(\"adult.test\", sep=',', \n",
    "                         header=None, names=['age','workclass','fnlwgt','education','education-num', 'marital-status', 'occupation',\n",
    "                                             'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "                                             'native-country', 'labels'])\n",
    "num_classes = 2\n",
    "adult_test = adult_test.drop([0])\n",
    "adult_test = adult_test.reset_index(drop=True)\n",
    "adult_test[['age']] = adult_test[['age']].astype('int64')\n",
    "adult_test[['fnlwgt']] = adult_test[['fnlwgt']].astype('int64')\n",
    "adult_test[['education-num']] = adult_test[['education-num']].astype('int64')\n",
    "adult_test[['capital-gain']] = adult_test[['capital-gain']].astype('int64')\n",
    "adult_test[['capital-loss']] = adult_test[['capital-loss']].astype('int64')\n",
    "adult_test[['hours-per-week']] = adult_test[['hours-per-week']].astype('int64')\n",
    "adult_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
      "       'hours-per-week'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['workclass', 'education', 'marital-status', 'occupation',\n",
       "       'relationship', 'race', 'sex', 'native-country', 'labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_obj = adult_data.select_dtypes(include=[object])\n",
    "train_val = adult_data.select_dtypes(include=['int64'])\n",
    "train_val = np.array(train_val)\n",
    "\n",
    "test_obj = adult_test.select_dtypes(include=[object])\n",
    "test_val = adult_test.select_dtypes(include=['int64'])\n",
    "print(test_val.columns)\n",
    "test_val = np.array(test_val)\n",
    "\n",
    "test_obj = test_obj.dropna()\n",
    "test_obj.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 9)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_obj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' <=50K.', ' >50K.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_features = preprocessing.LabelEncoder()\n",
    "train_obj = train_obj.apply(le_features.fit_transform)\n",
    "test_obj = test_obj.apply(le_features.fit_transform)\n",
    "list(le_features.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hmh222199\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "enc_labels = preprocessing.OneHotEncoder()\n",
    "\n",
    "enc_labels.fit(train_obj)\n",
    "train_onehotlabels = enc_labels.transform(train_obj).toarray()\n",
    "\n",
    "test_onehotlabels = enc_labels.transform(test_obj).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 102)\n",
      "(16281, 102)\n",
      "(32561, 108)\n",
      "(16281, 108)\n"
     ]
    }
   ],
   "source": [
    "train_labels_onehotlabels = train_onehotlabels[:, -2:]\n",
    "train_features_onehotlabels = train_onehotlabels[:, :-2]\n",
    "\n",
    "train_fea = np.hstack([train_features_onehotlabels, train_val])\n",
    "train_lab = train_labels_onehotlabels\n",
    "\n",
    "test_labels_onehotlabels = test_onehotlabels[:, -2:]\n",
    "test_features_onehotlabels = test_onehotlabels[:, :-2]\n",
    "print(train_features_onehotlabels.shape)\n",
    "print(test_features_onehotlabels.shape)\n",
    "test_fea = np.hstack([test_features_onehotlabels, test_val])\n",
    "test_lab = test_labels_onehotlabels\n",
    "\n",
    "print(train_fea.shape)\n",
    "print(test_fea.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#NN parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 1000\n",
    "batch_size = 100\n",
    "display_steps = 20\n",
    "beta = 0\n",
    "\n",
    "num_input = 108\n",
    "num_hidden1 = 500\n",
    "num_hidden2 = 500\n",
    "num_hidden3 = 500\n",
    "num_classes = 2\n",
    "dropout = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset():\n",
    "    def __init__(self, features, labels, batch_size):\n",
    "        self.fea_batches = []\n",
    "        self.lab_batches = []\n",
    "        self.batch_size = batch_size\n",
    "        self.size = len(features)\n",
    "        i = 0\n",
    "        while(i < self.size - batch_size):\n",
    "            self.fea_batches.append(features[i:i+batch_size])\n",
    "            self.lab_batches.append(labels[i:i+ batch_size])\n",
    "            i+= batch_size\n",
    "        self.idx = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        self.idx += 1\n",
    "        if (self.idx == self.size // self.batch_size):\n",
    "            self.idx = 0\n",
    "        return self.fea_batches[self.idx], self.lab_batches[self.idx]\n",
    "    \n",
    "\n",
    "train_data = dataset(train_fea, train_lab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float64, shape=[None, num_input])\n",
    "Y = tf.placeholder(tf.float64, shape=[None, num_classes])\n",
    "\n",
    "def neural_net(x, weights, biases, dropout):\n",
    "    #reshape x back to be an image\n",
    "    #x = tf.reshape(x, [-1, 28, 28 ,1])\n",
    "    \n",
    "    layer1 = tf.matmul(x, weights['w1']) + biases['b1']\n",
    "    layer1 = tf.contrib.nn.alpha_dropout(tf.nn.selu(layer1), dropout)\n",
    "    \n",
    "    layer2 = tf.matmul(layer1, weights['w2']) + biases['b2']\n",
    "    layer2 = tf.contrib.nn.alpha_dropout(tf.nn.selu(layer2), dropout)\n",
    "    \n",
    "    layer3 = tf.matmul(layer2, weights['w3']) + biases['b3']\n",
    "    layer3 = tf.contrib.nn.alpha_dropout(tf.nn.selu(layer3), dropout)\n",
    "    \n",
    "    output = tf.matmul(layer3, weights['w_out']) + biases['b_out']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'w1': tf.get_variable('w1', shape=[num_input, num_hidden1], dtype=tf.float64, initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_IN')),\n",
    "    'w2': tf.get_variable('w2', shape=[num_hidden1, num_hidden2],dtype=tf.float64, initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_IN')),\n",
    "    'w3': tf.get_variable('w3', shape=[num_hidden2, num_hidden3],dtype=tf.float64, initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_IN')),\n",
    "    'w_out': tf.get_variable('w_out', shape=[num_hidden3, num_classes], dtype=tf.float64,initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_IN'))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.get_variable('b1', shape=[num_hidden1],dtype=tf.float64, initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_IN')),\n",
    "    'b2': tf.get_variable('b2', shape=[num_hidden2],dtype=tf.float64, initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_IN')),\n",
    "    'b3': tf.get_variable('b3', shape=[num_hidden3],dtype=tf.float64, initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_IN')),\n",
    "    'b_out': tf.get_variable('b_out', shape=[num_classes],dtype=tf.float64, initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_IN'))\n",
    "}\n",
    "\n",
    "\n",
    "logits = neural_net(X, weights, biases, dropout)\n",
    "\n",
    "regularizer = tf.nn.l2_loss(weights['w1']) + tf.nn.l2_loss(weights['w2']) + tf.nn.l2_loss(weights['w_out'])\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y) + beta * regularizer)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step20, Minibatch loss 59.4271, Trainning accuracy 0.510\n",
      "Step40, Minibatch loss 8.4485, Trainning accuracy 0.650\n",
      "Step60, Minibatch loss 3.4815, Trainning accuracy 0.790\n",
      "Step80, Minibatch loss 1.7394, Trainning accuracy 0.750\n",
      "Step100, Minibatch loss 0.7620, Trainning accuracy 0.820\n",
      "Step120, Minibatch loss 0.8093, Trainning accuracy 0.790\n",
      "Step140, Minibatch loss 1.0417, Trainning accuracy 0.750\n",
      "Step160, Minibatch loss 0.5906, Trainning accuracy 0.780\n",
      "Step180, Minibatch loss 0.6080, Trainning accuracy 0.710\n",
      "Step200, Minibatch loss 0.6101, Trainning accuracy 0.740\n",
      "Step220, Minibatch loss 0.6413, Trainning accuracy 0.800\n",
      "Step240, Minibatch loss 0.5999, Trainning accuracy 0.730\n",
      "Step260, Minibatch loss 0.6472, Trainning accuracy 0.690\n",
      "Step280, Minibatch loss 0.5641, Trainning accuracy 0.750\n",
      "Step300, Minibatch loss 0.5292, Trainning accuracy 0.760\n",
      "Step320, Minibatch loss 0.5540, Trainning accuracy 0.760\n",
      "Step340, Minibatch loss 0.4997, Trainning accuracy 0.800\n",
      "Step360, Minibatch loss 0.6372, Trainning accuracy 0.730\n",
      "Step380, Minibatch loss 0.6200, Trainning accuracy 0.710\n",
      "Step400, Minibatch loss 0.5346, Trainning accuracy 0.770\n",
      "Step420, Minibatch loss 0.4726, Trainning accuracy 0.830\n",
      "Step440, Minibatch loss 0.5850, Trainning accuracy 0.770\n",
      "Step460, Minibatch loss 0.6587, Trainning accuracy 0.720\n",
      "Step480, Minibatch loss 0.6007, Trainning accuracy 0.720\n",
      "Step500, Minibatch loss 0.5247, Trainning accuracy 0.780\n",
      "Step520, Minibatch loss 0.5676, Trainning accuracy 0.780\n",
      "Step540, Minibatch loss 0.5929, Trainning accuracy 0.720\n",
      "Step560, Minibatch loss 0.6557, Trainning accuracy 0.660\n",
      "Step580, Minibatch loss 0.4624, Trainning accuracy 0.840\n",
      "Step600, Minibatch loss 0.5546, Trainning accuracy 0.810\n",
      "Step620, Minibatch loss 0.6265, Trainning accuracy 0.750\n",
      "Step640, Minibatch loss 0.5927, Trainning accuracy 0.720\n",
      "Step660, Minibatch loss 0.6005, Trainning accuracy 0.730\n",
      "Step680, Minibatch loss 0.5803, Trainning accuracy 0.720\n",
      "Step700, Minibatch loss 0.5393, Trainning accuracy 0.770\n",
      "Step720, Minibatch loss 0.5776, Trainning accuracy 0.730\n",
      "Step740, Minibatch loss 0.6135, Trainning accuracy 0.710\n",
      "Step760, Minibatch loss 0.6440, Trainning accuracy 0.690\n",
      "Step780, Minibatch loss 0.4773, Trainning accuracy 0.820\n",
      "Step800, Minibatch loss 0.5232, Trainning accuracy 0.790\n",
      "Step820, Minibatch loss 0.6582, Trainning accuracy 0.750\n",
      "Step840, Minibatch loss 0.5977, Trainning accuracy 0.770\n",
      "Step860, Minibatch loss 0.5725, Trainning accuracy 0.740\n",
      "Step880, Minibatch loss 0.4820, Trainning accuracy 0.810\n",
      "Step900, Minibatch loss 0.5087, Trainning accuracy 0.800\n",
      "Step920, Minibatch loss 0.5495, Trainning accuracy 0.760\n",
      "Step940, Minibatch loss 0.5369, Trainning accuracy 0.770\n",
      "Step960, Minibatch loss 0.5804, Trainning accuracy 0.740\n",
      "Step980, Minibatch loss 0.5171, Trainning accuracy 0.790\n",
      "Step1000, Minibatch loss 0.5636, Trainning accuracy 0.740\n",
      "Optimization done!\n",
      "Test accuracy: 0.76561636\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = train_data.next_batch()\n",
    "\n",
    "        sess.run(train_op, feed_dict = {X:batch_x, Y:batch_y})\n",
    "\n",
    "        if step % display_steps == 0:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict = {X:batch_x, Y:batch_y})\n",
    "            print(\"Step\" + str(step) + \", Minibatch loss \" +\"{:.4f}\".format(loss) + \", Trainning accuracy \" + \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization done!\")\n",
    "\n",
    "    test_accu = sess.run(accuracy, feed_dict = {X:test_fea , Y:test_lab})\n",
    "\n",
    "    print(\"Test accuracy:\", test_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
