{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data\\train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "dropout_input = 0.8\n",
    "dropout_hidden = 0.8\n",
    "batch_size = 256\n",
    "num_steps = 7000\n",
    "display_steps = 200\n",
    "beta = 0\n",
    "\n",
    "num_input = 28*28\n",
    "num_hidden1 = 500\n",
    "num_hidden2 = 500\n",
    "num_hidden3 = 500\n",
    "num_layers = 3\n",
    "num_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x, weights, biases, dropout_input, dropout_hidden):\n",
    "    #reshape x back to be an image\n",
    "    #x = tf.reshape(x, [-1, 28, 28 ,1])\n",
    "    \n",
    "    layer1 = tf.matmul(x, weights['w1']) + biases['b1']\n",
    "    layer1 = tf.nn.dropout(tf.nn.relu(layer1), dropout_input)\n",
    "    \n",
    "    layer2 = tf.matmul(layer1, weights['w2']) + biases['b2']\n",
    "    layer2 = tf.nn.dropout(tf.nn.relu(layer2), dropout_hidden)\n",
    "    \n",
    "    layer3 = tf.matmul(layer2, weights['w3']) + biases['b3']\n",
    "    layer3 = tf.nn.dropout(tf.nn.relu(layer3), dropout_hidden)\n",
    "    \n",
    "    output = tf.matmul(layer3, weights['w_out']) + biases['b_out']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'w1': tf.Variable(tf.random_normal([num_input, num_hidden1])),\n",
    "    'w2': tf.Variable(tf.random_normal([num_hidden1, num_hidden2])),\n",
    "    'w3': tf.Variable(tf.random_normal([num_hidden2, num_hidden3])),\n",
    "    'w_out': tf.Variable(tf.random_normal([num_hidden3, num_classes])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([num_hidden1])),\n",
    "    'b2': tf.Variable(tf.random_normal([num_hidden2])),\n",
    "    'b3': tf.Variable(tf.random_normal([num_hidden3])),\n",
    "    'b_out': tf.Variable(tf.random_normal([num_classes])),\n",
    "}\n",
    "\n",
    "logits = neural_net(X, weights, biases, dropout_input, dropout_hidden)\n",
    "\n",
    "regularizer = tf.nn.l2_loss(weights['w1']) + tf.nn.l2_loss(weights['w2']) + tf.nn.l2_loss(weights['w_out'])\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y) + beta * regularizer)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1, Minibatch loss51057.6836, Trainning accuracy0.145\n",
      "Step200, Minibatch loss883.1470, Trainning accuracy0.879\n",
      "Step400, Minibatch loss385.4158, Trainning accuracy0.891\n",
      "Step600, Minibatch loss219.8822, Trainning accuracy0.895\n",
      "Step800, Minibatch loss98.0753, Trainning accuracy0.898\n",
      "Step1000, Minibatch loss145.4314, Trainning accuracy0.875\n",
      "Step1200, Minibatch loss54.8341, Trainning accuracy0.883\n",
      "Step1400, Minibatch loss47.9446, Trainning accuracy0.801\n",
      "Step1600, Minibatch loss22.1117, Trainning accuracy0.637\n",
      "Step1800, Minibatch loss4.8061, Trainning accuracy0.488\n",
      "Step2000, Minibatch loss2.7673, Trainning accuracy0.352\n",
      "Step2200, Minibatch loss3.0988, Trainning accuracy0.293\n",
      "Step2400, Minibatch loss4.3245, Trainning accuracy0.336\n",
      "Step2600, Minibatch loss2.1627, Trainning accuracy0.316\n",
      "Step2800, Minibatch loss2.0585, Trainning accuracy0.320\n",
      "Step3000, Minibatch loss2.4539, Trainning accuracy0.266\n",
      "Step3200, Minibatch loss1.7789, Trainning accuracy0.301\n",
      "Step3400, Minibatch loss1.7991, Trainning accuracy0.309\n",
      "Step3600, Minibatch loss1.8543, Trainning accuracy0.305\n",
      "Step3800, Minibatch loss1.8211, Trainning accuracy0.348\n",
      "Step4000, Minibatch loss1.8668, Trainning accuracy0.242\n",
      "Step4200, Minibatch loss2.1796, Trainning accuracy0.238\n",
      "Step4400, Minibatch loss1.7294, Trainning accuracy0.363\n",
      "Step4600, Minibatch loss2.9011, Trainning accuracy0.340\n",
      "Step4800, Minibatch loss3.6638, Trainning accuracy0.320\n",
      "Step5000, Minibatch loss1.8501, Trainning accuracy0.266\n",
      "Step5200, Minibatch loss1.9666, Trainning accuracy0.254\n",
      "Step5400, Minibatch loss2.7452, Trainning accuracy0.371\n",
      "Step5600, Minibatch loss1.6303, Trainning accuracy0.371\n",
      "Step5800, Minibatch loss3.1906, Trainning accuracy0.406\n",
      "Step6000, Minibatch loss1.3425, Trainning accuracy0.496\n",
      "Step6200, Minibatch loss1.6492, Trainning accuracy0.391\n",
      "Step6400, Minibatch loss1.6489, Trainning accuracy0.434\n",
      "Step6600, Minibatch loss2.0317, Trainning accuracy0.469\n",
      "Step6800, Minibatch loss1.4422, Trainning accuracy0.473\n",
      "Step7000, Minibatch loss1.4728, Trainning accuracy0.410\n",
      "Optimization done!\n",
      "Test accuracy: 0.4556\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        sess.run(train_op, feed_dict = {X: batch_x, Y:batch_y})\n",
    "        \n",
    "        if step % display_steps == 0 or step == 1:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict = {X: batch_x, Y:batch_y})\n",
    "            print(\"Step\" + str(step) + \", Minibatch loss\" +\"{:.4f}\".format(loss) + \", Trainning accuracy\" + \"{:.3f}\".format(acc))\n",
    "    \n",
    "    print(\"Optimization done!\")\n",
    "    \n",
    "    test_accu = sess.run(accuracy, feed_dict = {X: mnist.test.images, Y:mnist.test.labels})\n",
    "    \n",
    "    print(\"Test accuracy:\", test_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
